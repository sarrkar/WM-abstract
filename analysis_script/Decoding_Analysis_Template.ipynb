{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5d9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper import read_data, frame_based_corrected_trials, concat_nback_decoder_data_gen, nback_decoder_cnn_data_gen, decoder, nback_decoder_data_gen\n",
    "save_path = \"/home/xiaoxuan/projects/multfs_triple/evaluation/Organized_analysis/results/\"\n",
    "\n",
    "import matplotlib as mpl \n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 14, \"ytick.major.size\": 14})\n",
    "sns.set_context(\"poster\")\n",
    "mpl.rcParams['axes.linewidth']=2.5\n",
    "mpl.rcParams['ytick.major.width']=2.5\n",
    "mpl.rcParams['xtick.major.width']=2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_based_corrected_trials(df, is_balanced = True):\n",
    "    \"return trials with corrected trials across all frames, if is_balanced is True, subsample to the smallest trials of all tasks\"\n",
    "    task_index_list = df.plain_task_index.unique()\n",
    "    update_dfs = [] \n",
    "    min_len_df = 100000\n",
    "\n",
    "    for i, task_index in enumerate(task_index_list):\n",
    "        curr_df = df[df[\"plain_task_index\"] == task_index]\n",
    "        selected_trials = [i for i in range(len(curr_df)) if (curr_df.predicted_action.iloc[i] == curr_df.corrected_action.iloc[i]).all()]\n",
    "        print(\"length of selected_trials:\", len(selected_trials))\n",
    "        curr_df = curr_df.iloc[selected_trials]\n",
    "        update_dfs.append(curr_df)\n",
    "\n",
    "        if min_len_df > len(curr_df):\n",
    "            min_len_df = len(curr_df)\n",
    "    \n",
    "    if is_balanced: # subsample each df to min_len_df\n",
    "        for i, curr_df in enumerate(update_dfs):\n",
    "            curr_df = curr_df.sample(n=min_len_df, random_state=42)\n",
    "            update_dfs[i] = curr_df\n",
    "\n",
    "    return pd.concat(update_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6848997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate across multiple df but only store the relevant information\n",
    "\n",
    "basepath_list = [\n",
    "        \"/mnt/store1/xiaoxuan/multfs_triple/eval/experiment_logs/2024611/RNN_rank256_init\" # change it to the path you need\n",
    "                ]\n",
    "\n",
    "\n",
    "model_type = \"RNN\"\n",
    "small_dataset = False\n",
    "task_name = \"nback\"\n",
    "\n",
    "mode = \"val_angle\"\n",
    "basepath = basepath_list[0]\n",
    "if not small_dataset:\n",
    "    from decoding_helper import read_data\n",
    "else:\n",
    "\n",
    "    for basepath in basepath_list:\n",
    "        task_name, df = read_data(basepath, mode = mode)\n",
    "        break\n",
    "    \n",
    "\n",
    "\n",
    "if small_dataset:\n",
    "    RNN_activations = []\n",
    "    CNN_activations = []\n",
    "    ntask_indices = []\n",
    "    feature_indices = []\n",
    "    loc_labels = []\n",
    "    obj_labels = []\n",
    "    ctg_labels = []\n",
    "    df = trial_based_corrected_trials(df,is_balanced = True)\n",
    "    df = df.sample(frac=1)\n",
    "    # if with RNN and GRU\n",
    "    RNN_activation = np.stack(df.activation.to_numpy())[:,0,:]\n",
    "    # if with LSTM cell_state\n",
    "    # RNN_activation = np.stack(df.cell_state.to_numpy())[:,0,:]\n",
    "    # if with LSTM hidden_state\n",
    "    # RNN_activation = np.stack(df.hidden_state.to_numpy())[:,0,:]\n",
    "\n",
    "    CNN_activation = np.stack(df.CNN_activation_2.to_numpy())[:,0,:,:,:].reshape(-1, 256*7*7)\n",
    "    ntask_index = df.ntask_index.to_numpy()\n",
    "    feature_index = df.feature_index.to_numpy()\n",
    "    loc_label = np.squeeze(np.stack(df.input_loc.to_numpy())[:,0])\n",
    "    obj_label = np.squeeze(np.stack(df.input_obj.to_numpy())[:,0])\n",
    "    ctg_label = np.squeeze(np.stack(df.input_ctg.to_numpy())[:,0])\n",
    "\n",
    "    del df\n",
    "\n",
    "    RNN_activations.append(RNN_activation)\n",
    "    CNN_activations.append(CNN_activation)\n",
    "    ntask_indices.append(ntask_index)\n",
    "    feature_indices.append(feature_index)\n",
    "    loc_labels.append(loc_label)\n",
    "    obj_labels.append(obj_label)\n",
    "    ctg_labels.append(ctg_label)\n",
    "    RNN_activation = np.concatenate(RNN_activations, axis = 0)\n",
    "    CNN_activation = np.concatenate(CNN_activations, axis = 0)\n",
    "    ntask_index = np.concatenate(ntask_indices, axis = 0)\n",
    "    feature_index = np.concatenate(feature_indices, axis = 0)\n",
    "    loc_label = np.concatenate(loc_labels, axis = 0)\n",
    "    obj_label = np.concatenate(obj_labels, axis = 0)\n",
    "    ctg_label = np.concatenate(ctg_labels, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure there are 9 tasks in toal\n",
    "if small_dataset:\n",
    "    assert np.max(ntask_index) == 3\n",
    "    assert np.min(ntask_index) == 1\n",
    "    assert np.max(feature_index) == 2\n",
    "    assert np.min(feature_index) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not small_dataset:\n",
    "    # iterate over all pickle files and collect all possible nback ns and feature index\n",
    "    feature_nback_mapping = {}\n",
    "\n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(basepath_list[0]):\n",
    "        # Check if the filename matches the pattern\n",
    "        if filename.endswith(\"_activations.csv\"):\n",
    "            # Parse the nback and feature indices from the filename\n",
    "            parts = filename.split(\"_\")\n",
    "            nback_index = int(parts[0].replace(\"back\", \"\"))\n",
    "            feature_index = int(parts[2].replace(\"feature\", \"\"))\n",
    "\n",
    "            # Update the dictionary\n",
    "            if feature_index not in feature_nback_mapping:\n",
    "                feature_nback_mapping[feature_index] = set()\n",
    "            feature_nback_mapping[feature_index].add(nback_index)\n",
    "\n",
    "    print(\"Feature-Nback Mapping:\")\n",
    "    for feature_index, nback_indices in feature_nback_mapping.items():\n",
    "        print(\"Feature\", feature_index, \": Possible nback indices:\", nback_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1e948d",
   "metadata": {},
   "source": [
    "# Single task decoding analysis\n",
    "single-task (N) single-feature (L,I,C) model:\n",
    "\t\tfor each stingle task model (3 in total), each row of the matrix shows the performance of the decoder on the vallidation dataset. \n",
    "\tthe point to be made is :\n",
    "\t\twhether the single task networks retain or forget the task irrelevant feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_nback_decoder_data_gen(RNN_activation,ntask_index, selected_ntask_index, feature_index, selected_feature_index, decoding_feature_labels, split_ratio = 0.8):\n",
    "    # get [0.8,0.2] splited activation for selected task at frames on frame_list\n",
    "    indice_1 = np.where(ntask_index == selected_ntask_index)\n",
    "    indice_2 = np.where(feature_index == selected_feature_index)\n",
    "    indice = np.intersect1d(indice_1, indice_2)\n",
    "    # randomize indices\n",
    "    np.random.shuffle(indice)\n",
    "    \n",
    "    curr_data = RNN_activation[indice]\n",
    "    # subtract the mean of the RNN activation\n",
    "    curr_data = curr_data - np.mean(curr_data, axis = 0)\n",
    "    print(\"curr data shape:\", curr_data.shape)\n",
    "    \n",
    "    curr_label = decoding_feature_labels[indice]\n",
    "    l = curr_data.shape[0]\n",
    "    l_train = int(np.floor(l*split_ratio))\n",
    "\n",
    "    train_data = curr_data[:l_train]\n",
    "    val_data = curr_data[l_train:]\n",
    "    train_label = curr_label[:l_train]\n",
    "    val_label = curr_label[l_train:]\n",
    "\n",
    "    return train_data, train_label, val_data, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: save 9 datapoints\n",
    "if small_dataset:\n",
    "    task_feature_index = 0\n",
    "    accs = np.zeros((1,3)) # rows: L,I,C 1back model, columns: decoding for loc, id, ctg feature\n",
    "\n",
    "    classifier_type = \"svm_linear\"\n",
    "    labels = [loc_label, obj_label, ctg_label]\n",
    "    n_bootstraps = 1\n",
    "\n",
    "    for selected_train_ntask_index in [3]: # only consider 1back task here\n",
    "        for selected_train_feature_index in [task_feature_index]:\n",
    "            for i in range(n_bootstraps):\n",
    "                # within the same task\n",
    "                selected_val_ntask_index = selected_train_ntask_index\n",
    "                selected_val_feature_index = selected_train_feature_index\n",
    "\n",
    "                # test for location\n",
    "                for decoding_feature in range(3):\n",
    "                    decoding_feature_labels = labels[decoding_feature]\n",
    "\n",
    "                    train_data, train_label, val_data, val_label = concat_nback_decoder_data_gen(RNN_activation, \n",
    "                                                                                                 ntask_index,\n",
    "                                                                                                 selected_train_ntask_index, \n",
    "                                                                                                 feature_index, \n",
    "                                                                                                 selected_train_feature_index, \n",
    "                                                                                                 decoding_feature_labels, \n",
    "                                                                                                 split_ratio = 0.8)\n",
    "\n",
    "                    acc= decoder(train_data, val_data, train_label, val_label, type = classifier_type)\n",
    "                    print(\"decoding accuracy:\", acc)\n",
    "                    accs[0, decoding_feature] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a2aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information retention analysis: goal: save 9 datapoints\n",
    "if small_dataset:\n",
    "    accs = np.zeros((3,3)) # rows: L,I,C 1back model, columns: decoding for loc, id, ctg feature\n",
    "\n",
    "    classifier_type = \"svm_linear\"\n",
    "    labels = [loc_label, obj_label, ctg_label]\n",
    "    n_bootstraps = 10\n",
    "\n",
    "    for selected_train_ntask_index in [1]: # only consider 1back task here\n",
    "        for selected_train_feature_index in range(3):\n",
    "            for i in range(n_bootstraps):\n",
    "                for decoding_feature in range(3):\n",
    "\n",
    "                    selected_val_ntask_index = selected_train_ntask_index\n",
    "\n",
    "\n",
    "\n",
    "                    decoding_feature_labels = labels[decoding_feature]\n",
    "\n",
    "                    train_data, train_label, val_data, val_label= concat_nback_decoder_data_gen(RNN_activation, \n",
    "                                                                                 ntask_index,\n",
    "                                                                                 selected_train_ntask_index, \n",
    "                                                                                 feature_index, \n",
    "                                                                                 selected_train_feature_index, \n",
    "                                                                                 decoding_feature_labels, \n",
    "                                                                                 split_ratio = 0.8)\n",
    "\n",
    "\n",
    "\n",
    "                    acc= decoder(train_data, val_data, train_label, val_label, type = classifier_type)\n",
    "                    print(\"decoding accuracy:\", acc)\n",
    "                    accs[selected_train_feature_index, decoding_feature] = acc\n",
    "    print(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f38740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# information retention analysis: goal: save 9 datapoints\n",
    "if not small_dataset:\n",
    "    n_bootstraps = 2\n",
    "    accs = np.zeros((3,3,3, n_bootstraps)) #(nback index, task feature, decoding feature)\n",
    "\n",
    "    classifier_type = \"svm_linear\"\n",
    "    \n",
    "    for task_feature in feature_nback_mapping.keys():\n",
    "        for nback_n in feature_nback_mapping[task_feature]:\n",
    "\n",
    "            decoding_feature = task_feature\n",
    "            # obtain the task relevant data\n",
    "            task_name, df = read_data(basepath, path2file = basepath + \"/%dback_feature_%d_activations.pkl\" % (nback_n, task_feature), mode = mode)\n",
    "            n_feature_values = []\n",
    "            n_feature_values.append(list(set(np.stack(df.input_loc.to_numpy()).reshape(-1))))\n",
    "            n_feature_values.append(list(set(np.stack(df.input_obj.to_numpy()).reshape(-1))))\n",
    "            n_feature_values.append(list(set(np.stack(df.input_ctg.to_numpy()).reshape(-1))))\n",
    "\n",
    "            RNN_activations = []\n",
    "            CNN_activations = []\n",
    "            ntask_indices = []\n",
    "            feature_indices = []\n",
    "            loc_labels = []\n",
    "            obj_labels = []\n",
    "            ctg_labels = []\n",
    "            df = trial_based_corrected_trials(df,is_balanced = True)\n",
    "            df = df.sample(frac=1)\n",
    "            # if with RNN and GRU\n",
    "            RNN_activation = np.stack(df.activation.to_numpy())[:,0,:]\n",
    "            # if with LSTM cell_state\n",
    "            # RNN_activation = np.stack(df.cell_state.to_numpy())[:,0,:]\n",
    "            # if with LSTM hidden_state\n",
    "            # RNN_activation = np.stack(df.hidden_state.to_numpy())[:,0,:]\n",
    "\n",
    "            CNN_activation = np.stack(df.CNN_activation_2.to_numpy())[:,0,:,:,:].reshape(-1, 256*7*7)\n",
    "            ntask_index = df.ntask_index.to_numpy()\n",
    "            feature_index = df.feature_index.to_numpy()\n",
    "            loc_label = np.squeeze(np.stack(df.input_loc.to_numpy())[:,0])\n",
    "            obj_label = np.squeeze(np.stack(df.input_obj.to_numpy())[:,0])\n",
    "            ctg_label = np.squeeze(np.stack(df.input_ctg.to_numpy())[:,0])\n",
    "\n",
    "            del df\n",
    "\n",
    "            RNN_activations.append(RNN_activation)\n",
    "            CNN_activations.append(CNN_activation)\n",
    "            ntask_indices.append(ntask_index)\n",
    "            feature_indices.append(feature_index)\n",
    "            loc_labels.append(loc_label)\n",
    "            obj_labels.append(obj_label)\n",
    "            ctg_labels.append(ctg_label)\n",
    "            RNN_activation = np.concatenate(RNN_activations, axis = 0)\n",
    "            CNN_activation = np.concatenate(CNN_activations, axis = 0)\n",
    "            ntask_index = np.concatenate(ntask_indices, axis = 0)\n",
    "            feature_index = np.concatenate(feature_indices, axis = 0)\n",
    "            loc_label = np.concatenate(loc_labels, axis = 0)\n",
    "            obj_label = np.concatenate(obj_labels, axis = 0)\n",
    "            ctg_label = np.concatenate(ctg_labels, axis = 0)\n",
    "\n",
    "            labels = [loc_label, obj_label, ctg_label]\n",
    "            \n",
    "\n",
    "            selected_train_ntask_index = nback_n\n",
    "            selected_train_feature_index = task_feature\n",
    "            \n",
    "            for i in range(n_bootstraps):\n",
    "                for decoding_feature in range(3):\n",
    "\n",
    "                    selected_val_ntask_index = selected_train_ntask_index\n",
    "                    decoding_feature_labels = labels[decoding_feature]\n",
    "\n",
    "                    train_data, train_label, val_data, val_label= concat_nback_decoder_data_gen(RNN_activation, \n",
    "                                                                                 ntask_index,\n",
    "                                                                                 selected_train_ntask_index, \n",
    "                                                                                 feature_index, \n",
    "                                                                                 selected_train_feature_index, \n",
    "                                                                                 decoding_feature_labels, \n",
    "                                                                                 split_ratio = 0.8)\n",
    "\n",
    "\n",
    "\n",
    "                    acc= decoder(train_data, val_data, train_label, val_label, type = classifier_type)\n",
    "                    print(\"decoding accuracy:\", acc)\n",
    "                    accs[nback_n-1, selected_train_feature_index, decoding_feature,i] = acc\n",
    "    accs = np.mean(accs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d8148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
