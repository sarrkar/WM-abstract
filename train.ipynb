{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOdeOBqBp9-_"
      },
      "outputs": [],
      "source": [
        "!unzip abstract.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e__p9M1SXBz8"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision matplotlib sympy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jGfQYVqELa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tasks import DMSDataset, OneBackDataset, TwoBackDataset, ThreeBackDataset, CtxDMSDataset, InterDMSDataset\n",
        "from models.RNN_model import CustomRNN\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=2000, learning_rate=0.001, verbose=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Lists to track loss and accuracy\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_action_accuracy = 0.0\n",
        "        running_no_action_accuracy = 0.0\n",
        "        running_val_action_accuracy = 0.0\n",
        "        running_val_no_action_accuracy = 0.0\n",
        "        total_train_batches = 0\n",
        "        total_val_batches = 0\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for i, (inputs, labels, task_index) in enumerate(train_dataloader):\n",
        "            inputs, labels, task_index = inputs, labels, task_index\n",
        "\n",
        "            # Extend task_index and concatenate with inputs\n",
        "            task_index_extended = task_index.unsqueeze(1)\n",
        "            task_index_repeated = task_index_extended.repeat(1, inputs.shape[1], 1)\n",
        "            concatenated = torch.cat((inputs, task_index_repeated), dim=-1).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(concatenated)\n",
        "            softmax_outputs = F.softmax(outputs, dim=-1)\n",
        "            predicted_actions = torch.argmax(softmax_outputs, dim=-1)\n",
        "\n",
        "            # Calculate separate action and no-action accuracies\n",
        "            action_mask = (labels == 0) | (labels == 1)\n",
        "            no_action_mask = (labels == 2)\n",
        "\n",
        "            action_correct = (predicted_actions[action_mask] == labels[action_mask]).float().sum()\n",
        "            action_total = action_mask.float().sum()\n",
        "            action_accuracy = (action_correct / action_total).item() if action_total > 0 else 0.0\n",
        "\n",
        "            no_action_correct = (predicted_actions[no_action_mask] == labels[no_action_mask]).float().sum()\n",
        "            no_action_total = no_action_mask.float().sum()\n",
        "            no_action_accuracy = (no_action_correct / no_action_total).item() if no_action_total > 0 else 0.0\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1).long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_action_accuracy += action_accuracy\n",
        "            running_no_action_accuracy += no_action_accuracy\n",
        "            total_train_batches += 1\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        for i, (inputs, labels, task_index) in enumerate(val_dataloader):\n",
        "            inputs, labels, task_index = inputs, labels, task_index\n",
        "\n",
        "            task_index_extended = task_index.unsqueeze(1)\n",
        "            task_index_repeated = task_index_extended.repeat(1, inputs.shape[1], 1)\n",
        "            concatenated = torch.cat((inputs, task_index_repeated), dim=-1).float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs, _ = model(concatenated)\n",
        "            softmax_outputs = F.softmax(outputs, dim=-1)\n",
        "            predicted_actions = torch.argmax(softmax_outputs, dim=-1)\n",
        "\n",
        "            # Calculate separate action and no-action accuracies\n",
        "            action_mask = (labels == 0) | (labels == 1)\n",
        "            no_action_mask = (labels == 2)\n",
        "\n",
        "            action_correct = (predicted_actions[action_mask] == labels[action_mask]).float().sum()\n",
        "            action_total = action_mask.float().sum()\n",
        "            action_accuracy = (action_correct / action_total).item() if action_total > 0 else 0.0\n",
        "\n",
        "            no_action_correct = (predicted_actions[no_action_mask] == labels[no_action_mask]).float().sum()\n",
        "            no_action_total = no_action_mask.float().sum()\n",
        "            no_action_accuracy = (no_action_correct / no_action_total).item() if no_action_total > 0 else 0.0\n",
        "\n",
        "            running_val_action_accuracy += action_accuracy\n",
        "            running_val_no_action_accuracy += no_action_accuracy\n",
        "            total_val_batches += 1\n",
        "\n",
        "        # Calculate epoch statistics\n",
        "        epoch_loss = running_loss / total_train_batches\n",
        "        epoch_action_accuracy = running_action_accuracy / total_train_batches\n",
        "        epoch_no_action_accuracy = running_no_action_accuracy / total_train_batches\n",
        "\n",
        "        epoch_val_action_accuracy = running_val_action_accuracy / total_val_batches\n",
        "        epoch_val_no_action_accuracy = running_val_no_action_accuracy / total_val_batches\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_action_accuracy)\n",
        "        val_accuracies.append(epoch_val_action_accuracy)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '\n",
        "                  f'Action Accuracy: {epoch_action_accuracy * 100:.2f}%, '\n",
        "                  f'No Action Accuracy: {epoch_no_action_accuracy * 100:.2f}%, '\n",
        "                  f'Validation Action Accuracy: {epoch_val_action_accuracy * 100:.2f}%, '\n",
        "                  f'Validation No Action Accuracy: {epoch_val_no_action_accuracy * 100:.2f}%')\n",
        "\n",
        "        if len(accuracies) > 100 and np.mean(accuracies[-10:]) >= 0.99:\n",
        "            # Stop training if accuracy saturates\n",
        "            break\n",
        "\n",
        "    print(f'Final Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Loss: {epoch_loss:.4f}, '\n",
        "          f'Action Accuracy: {epoch_action_accuracy * 100:.2f}%, '\n",
        "          f'No Action Accuracy: {epoch_no_action_accuracy * 100:.2f}%, '\n",
        "          f'Validation Action Accuracy: {epoch_val_action_accuracy * 100:.2f}%, '\n",
        "          f'Validation No Action Accuracy: {epoch_val_no_action_accuracy * 100:.2f}%')\n",
        "\n",
        "    # Plot loss and accuracy\n",
        "    plt.figure(figsize=(5, 5))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(losses, label='Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(accuracies, label='Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(val_losses, label='Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(val_accuracies, label='Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "# Load the dataset and initialize DataLoader\n",
        "batch_size = 4096\n",
        "\n",
        "from itertools import product\n",
        "single_feature_datasets = {\n",
        "    f\"{prefix}_{feature}\": (dataset_class, {\"feature\": feature})\n",
        "    for prefix, dataset_class in [\n",
        "        (\"dms\", DMSDataset),\n",
        "        (\"oneback\", OneBackDataset),\n",
        "        (\"twoback\", TwoBackDataset),\n",
        "        (\"threeback\", ThreeBackDataset),\n",
        "    ]\n",
        "    for feature in [\"category\", \"identity\", \"position\"]\n",
        "}\n",
        "\n",
        "ctx_dms_datasets = {\n",
        "    'ctxdms_category_identity_position': (CtxDMSDataset,\n",
        "      {\"features\": [\"category\", \"identity\", \"position\"]}\n",
        "    ),\n",
        "    'ctxdms_position_category_identity': (CtxDMSDataset,\n",
        "      {\"features\": [\"position\", \"category\", \"identity\"]}\n",
        "    ),\n",
        "    'ctxdms_position_identity_category': (CtxDMSDataset,\n",
        "      {\"features\": [\"position\", \"identity\", \"category\"]}\n",
        "    ),\n",
        "    'ctxdms_identity_position_category': (CtxDMSDataset,\n",
        "      {\"features\": [\"identity\", \"position\", \"category\"]}\n",
        "    ),\n",
        "}\n",
        "\n",
        "inter_dms_datasets = {\n",
        "    f\"interdms_{pattern}_{feature1}_{feature2}\": (\n",
        "        InterDMSDataset,\n",
        "        {\"pattern\": pattern, \"features\": [feature1, feature2]},\n",
        "    )\n",
        "    for pattern in [\"AABB\", \"ABBA\", \"ABAB\"]\n",
        "    for feature1 in [\"category\", \"identity\", \"position\"]\n",
        "    for feature2 in [\"category\", \"identity\", \"position\"]\n",
        "}\n",
        "\n",
        "dataloaders = {**single_feature_datasets, **ctx_dms_datasets, **inter_dms_datasets}\n",
        "\n",
        "# Initialize the model with the desired RNN type\n",
        "input_size = 16 + 43 # Since each sequence element is a scalar, number of ids and task index lenght\n",
        "hidden_size = 4 # You can adjust the hidden layer size\n",
        "output_size = 3  # Three possible actions\n",
        "rnn_types = ['RNN']\n",
        "\n",
        "for name, (dataset_class, kwargs) in dataloaders.items():\n",
        "  train_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, add_noise=False, **kwargs)  # Noise during training\n",
        "  val_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, add_noise=False, **kwargs)  # No noise during validation\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "  for rnn_type in rnn_types:\n",
        "    print(rnn_type, name)\n",
        "    model = CustomRNN(input_size, hidden_size, output_size, rnn_type)\n",
        "    train_model(model, train_dataloader, val_dataloader, num_epochs=2, learning_rate=0.001)\n",
        "    plt.savefig(f'figs/{rnn_type}_{name}.png')\n",
        "  print('-' * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ud4zzDTXSQsz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=2000, learning_rate=0.001, verbose=False):\n",
        "    # Remove the weighted loss, using standard CrossEntropyLoss instead\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    task_names = [\n",
        "        f\"{prefix}_{feature}\"\n",
        "        for prefix in [\"dms\", \"oneback\", \"twoback\", \"threeback\"]\n",
        "        for feature in [\"category\", \"identity\", \"position\"]\n",
        "    ]\n",
        "    task_names += ['ctxdms_category_identity_position',\n",
        "                  'ctxdms_position_category_identity', 'ctxdms_position_identity_category', 'ctx_identity_position_category']\n",
        "    task_names += [\n",
        "        f\"interdms_{pattern}_{feature1}_{feature2}\"\n",
        "        for pattern in [\"AABB\", \"ABBA\", \"ABAB\"]\n",
        "        for feature1 in [\"category\", \"identity\", \"position\"]\n",
        "        for feature2 in [\"category\", \"identity\", \"position\"]\n",
        "    ]\n",
        "\n",
        "    # Initialize confusion matrices for each task\n",
        "    task_confusion_matrices = {task_name: np.zeros((3, 3), dtype=int) for task_name in task_names}\n",
        "    task_accuracies = {task_name: [] for task_name in task_names}\n",
        "    dms_confusion = {key: [[[] for _ in range(32)] for _ in range(32)] for key in task_names}\n",
        "    dms_embs = {key: [[] for _ in range(32)] for key in task_names}\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        running_action_accuracy = 0.0\n",
        "        running_no_action_accuracy = 0.0\n",
        "        running_val_loss = 0.0\n",
        "        running_val_action_accuracy = 0.0\n",
        "        running_val_no_action_accuracy = 0.0\n",
        "        total_train_batches = 0\n",
        "        total_val_batches = 0\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for i, (inputs, labels, task_index) in enumerate(train_dataloader):\n",
        "            inputs, labels, task_index = inputs.to('cuda'), labels.to('cuda'), task_index.to('cuda')\n",
        "\n",
        "            task_index_extended = task_index.unsqueeze(1)\n",
        "            task_index_repeated = task_index_extended.repeat(1, inputs.shape[1], 1)\n",
        "            concatenated = torch.cat((inputs, task_index_repeated), dim=-1)\n",
        "            concatenated = concatenated.float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs, _ = model(concatenated)\n",
        "\n",
        "            softmax_outputs = F.softmax(outputs, dim=-1)\n",
        "            predicted_actions = torch.argmax(softmax_outputs, dim=-1)\n",
        "\n",
        "            # Calculate action accuracy (for labels 0 and 1)\n",
        "            action_mask = (labels == 0) | (labels == 1)\n",
        "            no_action_mask = (labels == 2)\n",
        "\n",
        "            action_correct = (predicted_actions[action_mask] == labels[action_mask]).float().sum()\n",
        "            action_total = action_mask.float().sum()\n",
        "            action_accuracy = (action_correct / action_total).item() if action_total > 0 else 0.0\n",
        "\n",
        "            no_action_correct = (predicted_actions[no_action_mask] == labels[no_action_mask]).float().sum()\n",
        "            no_action_total = no_action_mask.float().sum()\n",
        "            no_action_accuracy = (no_action_correct / no_action_total).item() if no_action_total > 0 else 0.0\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1).long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_action_accuracy += action_accuracy\n",
        "            running_no_action_accuracy += no_action_accuracy\n",
        "            total_train_batches += 1\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        for i, (inputs, labels, task_index) in enumerate(val_dataloader):\n",
        "            inputs, labels, task_index = inputs.to('cuda'), labels.to('cuda'), task_index.to('cuda')\n",
        "\n",
        "            task_index_extended = task_index.unsqueeze(1)\n",
        "            task_index_repeated = task_index_extended.repeat(1, inputs.shape[1], 1)\n",
        "            concatenated = torch.cat((inputs, task_index_repeated), dim=-1)\n",
        "            concatenated = concatenated.float()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs, _ = model(concatenated)\n",
        "\n",
        "            softmax_outputs = F.softmax(outputs, dim=-1)\n",
        "            predicted_actions = torch.argmax(softmax_outputs, dim=-1)\n",
        "\n",
        "            action_correct = (predicted_actions[action_mask] == labels[action_mask]).float().sum()\n",
        "            action_total = action_mask.float().sum()\n",
        "            action_accuracy = (action_correct / action_total).item() if action_total > 0 else 0.0\n",
        "\n",
        "            no_action_correct = (predicted_actions[no_action_mask] == labels[no_action_mask]).float().sum()\n",
        "            no_action_total = no_action_mask.float().sum()\n",
        "            no_action_accuracy = (no_action_correct / no_action_total).item() if no_action_total > 0 else 0.0\n",
        "\n",
        "            running_val_action_accuracy += action_accuracy\n",
        "            running_val_no_action_accuracy += no_action_accuracy\n",
        "            total_val_batches += 1\n",
        "\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1).long())\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            for j in range(labels.shape[0]):\n",
        "              task_id = torch.argmax(task_index[j]).item()\n",
        "              task_name_str = task_names[task_id]\n",
        "              task_correct = (predicted_actions[j] == labels[j]).float().sum().item()\n",
        "              task_total = labels[j].shape[0]\n",
        "              task_accuracy = task_correct / task_total if task_total > 0 else 0.0\n",
        "              task_accuracies[task_name_str].append(task_accuracy)\n",
        "\n",
        "            print(\"Action Accuracies for all tasks:\")\n",
        "            for task_name, accuracies in task_accuracies.items():\n",
        "              avg_accuracy = np.mean(accuracies) if len(accuracies) > 0 else 0.0\n",
        "              print(f\"Task: {task_name}, Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "        # Epoch statistics\n",
        "        epoch_loss = running_loss / total_train_batches\n",
        "        epoch_action_accuracy = running_action_accuracy / total_train_batches\n",
        "        epoch_no_action_accuracy = running_no_action_accuracy / total_train_batches\n",
        "        epoch_val_loss = running_val_loss / total_val_batches\n",
        "        epoch_val_action_accuracy = running_val_action_accuracy / total_val_batches\n",
        "        epoch_val_no_action_accuracy = running_val_no_action_accuracy / total_val_batches\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_action_accuracy)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_action_accuracy)\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '\n",
        "                  f'Action Accuracy: {epoch_action_accuracy * 100:.2f}%, '\n",
        "                  f'No Action Accuracy: {epoch_no_action_accuracy * 100:.2f}%, '\n",
        "                  f'Validation Loss: {epoch_val_loss:.4f}, '\n",
        "                  f'Validation Action Accuracy: {epoch_val_action_accuracy * 100:.2f}%, '\n",
        "                  f'Validation No Action Accuracy: {epoch_val_no_action_accuracy * 100:.2f}%')\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, '\n",
        "          f'Action Accuracy: {epoch_action_accuracy * 100:.2f}%, '\n",
        "          f'No Action Accuracy: {epoch_no_action_accuracy * 100:.2f}%, '\n",
        "          f'Validation Loss: {epoch_val_loss:.4f}, '\n",
        "          f'Validation Action Accuracy: {epoch_val_action_accuracy * 100:.2f}%, '\n",
        "          f'Validation No Action Accuracy: {epoch_val_no_action_accuracy * 100:.2f}%')\n",
        "\n",
        "    avg_task_accuracies = {task: np.mean(acc) for task, acc in task_accuracies.items()}\n",
        "\n",
        "    # Plot bar chart of per-task accuracies\n",
        "    tasks = list(avg_task_accuracies.keys())\n",
        "    accuracies = list(avg_task_accuracies.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(tasks, accuracies)\n",
        "    plt.xlabel('Accuracy')\n",
        "    plt.ylabel('Task')\n",
        "    plt.title('Per-task Accuracy')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # Confusion matrices update for each task\n",
        "    for i, (inputs, labels, task_index) in enumerate(val_dataloader):\n",
        "        inputs, labels, task_index = inputs.to('cuda'), labels.to('cuda'), task_index.to('cuda')\n",
        "\n",
        "        task_index_extended = task_index.unsqueeze(1)\n",
        "        task_index_repeated = task_index_extended.repeat(1, inputs.shape[1], 1)\n",
        "        concatenated = torch.cat((inputs, task_index_repeated), dim=-1)\n",
        "        concatenated = concatenated.float()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, h = model(concatenated)\n",
        "\n",
        "        softmax_outputs = F.softmax(outputs, dim=-1)\n",
        "        predicted_actions = torch.argmax(softmax_outputs, dim=-1)\n",
        "\n",
        "        # Update confusion matrix for each task\n",
        "        for j in range(labels.shape[0]):\n",
        "            task_id = torch.argmax(task_index[j]).item()\n",
        "            task_name_str = task_names[task_id]\n",
        "            for k in range(labels.shape[1]):\n",
        "                true_label = labels[j][k].item()\n",
        "                predicted_label = predicted_actions[j][k].item()\n",
        "                task_confusion_matrices[task_name_str][int(true_label), int(predicted_label)] += 1\n",
        "\n",
        "            if task_name_str in [\n",
        "                'dms_category', 'dms_identity',\n",
        "                'interdms_AABB_category_category', 'interdms_AABB_category_identity', 'interdms_AABB_category_position',\n",
        "                'interdms_AABB_identity_category', 'interdms_AABB_identity_identity', 'interdms_AABB_identity_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][1][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'dms_position',\n",
        "                'interdms_AABB_position_category', 'interdms_AABB_position_identity', 'interdms_AABB_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][1][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in ['oneback_category', 'oneback_identity']:\n",
        "              for l in range(5):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+1][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+1][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+1][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str == 'oneback_position':\n",
        "              for l in range(5):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+1][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+1][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+1][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str in ['twoback_category', 'twoback_identity']:\n",
        "              for l in range(4):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+2][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str == 'twoback_position':\n",
        "              for l in range(4):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+2][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str in ['threeback_category', 'threeback_identity']:\n",
        "              for l in range(3):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+3][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str == 'threeback_position':\n",
        "              for l in range(3):\n",
        "                identity1 = torch.argmax(inputs[j][l][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][l+3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][l][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][l+3][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][l+3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][l].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_AABB_category_category', 'interdms_AABB_identity_category', 'interdms_AABB_position_category',\n",
        "                'interdms_AABB_category_identity', 'interdms_AABB_identity_identity', 'interdms_AABB_position_identity',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][2].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_AABB_category_position', 'interdms_AABB_identity_position', 'interdms_AABB_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][2].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABBA_category_category', 'interdms_ABBA_category_identity', 'interdms_ABBA_category_position',\n",
        "                'interdms_ABBA_identity_category', 'interdms_ABBA_identity_identity', 'interdms_ABBA_identity_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABBA_position_category', 'interdms_ABBA_position_identity', 'interdms_ABBA_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABBA_category_category', 'interdms_ABBA_identity_category', 'interdms_ABBA_position_category',\n",
        "                'interdms_ABBA_category_identity', 'interdms_ABBA_identity_identity', 'interdms_ABBA_position_identity',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][1].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABBA_category_position', 'interdms_ABBA_identity_position', 'interdms_ABBA_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][1].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABAB_category_category', 'interdms_ABAB_category_identity', 'interdms_ABAB_category_position',\n",
        "                'interdms_ABAB_identity_category', 'interdms_ABAB_identity_identity', 'interdms_ABAB_identity_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABAB_position_category', 'interdms_ABAB_position_identity', 'interdms_ABAB_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][0][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][2][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][0][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][2][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][2][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][0].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABAB_category_category', 'interdms_ABAB_identity_category', 'interdms_ABAB_position_category',\n",
        "                'interdms_ABAB_category_identity', 'interdms_ABAB_identity_identity', 'interdms_ABAB_position_identity',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = identity1 * 4 + position1\n",
        "                id2 = identity2 * 4 + position2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][1].detach().cpu().numpy())\n",
        "            if task_name_str in [\n",
        "                'interdms_ABAB_category_position', 'interdms_ABAB_identity_position', 'interdms_ABAB_position_position',\n",
        "                ]:\n",
        "                identity1 = torch.argmax(inputs[j][1][4:12]).item()\n",
        "                identity2 = torch.argmax(inputs[j][3][4:12]).item()\n",
        "                position1 = torch.argmax(inputs[j][1][12:16]).item()\n",
        "                position2 = torch.argmax(inputs[j][3][12:16]).item()\n",
        "                id1 = position1 * 8 + identity1\n",
        "                id2 = position2 * 8 + identity2\n",
        "                dms_confusion[task_name_str][id1][id2].append(softmax_outputs[j][3][1].item())\n",
        "                dms_embs[task_name_str][id1].append(h[j][1].detach().cpu().numpy())\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    for task_id, cm in task_confusion_matrices.items():\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'Confusion Matrix for Task {task_id}')\n",
        "        plt.show()\n",
        "\n",
        "# Inside the train_model function, after generating the confusion and embedding heatmaps:\n",
        "    for task in task_names:\n",
        "        lim = 32\n",
        "        for i in range(lim):\n",
        "            for j in range(lim):\n",
        "                if len(dms_confusion[task][i][j]) > 0:\n",
        "                    dms_confusion[task][i][j] = np.mean(dms_confusion[task][i][j])\n",
        "                else:\n",
        "                    dms_confusion[task][i][j] = 0\n",
        "        dms_confusion[task] = np.array(dms_confusion[task])\n",
        "\n",
        "        # Save the probability matrix for the current task in .npz format\n",
        "        np.savez_compressed(f'probability_matrices/{task}_probability_matrix.npz', matrix=dms_confusion[task])\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        sns.heatmap(dms_confusion[task])\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'Probability Matrix for Task {task}')\n",
        "        plt.show()\n",
        "\n",
        "        emb_heatmap = np.zeros((lim, lim))\n",
        "        for i in range(lim):\n",
        "            for j in range(lim):\n",
        "                emb1 = np.mean(dms_embs[task][i], axis=0)\n",
        "                emb2 = np.mean(dms_embs[task][j], axis=0)\n",
        "                emb_heatmap[i, j] = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "        # Save the embedding similarity heatmap for the current task in .npz format\n",
        "        np.savez_compressed(f'probability_matrices/{task}_embedding_similarity.npz', matrix=emb_heatmap)\n",
        "\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        sns.heatmap(emb_heatmap)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title(f'Cosine Similarity of {task} embeddings')\n",
        "        plt.show()\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(losses, label='Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(accuracies, label='Action Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training Action Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(val_accuracies, label='Validation Action Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Action Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "batch_size = 4096\n",
        "\n",
        "for name, (dataset_class, kwargs) in dataloaders.items():\n",
        "    train_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, pad_to=6, add_noise=True, **kwargs)\n",
        "    train_datasets.append(train_dataset)\n",
        "\n",
        "    val_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, pad_to=6, add_noise=True, **kwargs)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "# merge datasets\n",
        "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
        "val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "hidden_states = [256]\n",
        "\n",
        "for hidden_size in hidden_states:\n",
        "    for rnn_type in rnn_types:\n",
        "        print(rnn_type, hidden_size)\n",
        "        model = CustomRNN(input_size, hidden_size, output_size, rnn_type).to('cuda')\n",
        "        train_model(model, train_dataloader, val_dataloader, num_epochs=200, learning_rate=0.001, verbose=True)\n",
        "        plt.savefig(f'figs/{rnn_type}_{hidden_size}_mixed_0.png')\n",
        "        print('-' * 80)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Yxj6tKV2F73z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "batch_size = 4096\n",
        "\n",
        "for name, (dataset_class, kwargs) in dataloaders.items():\n",
        "    train_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, pad_to=6, add_noise=True, **kwargs)\n",
        "    train_datasets.append(train_dataset)\n",
        "\n",
        "    val_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0.15, pad_to=6, add_noise=True, **kwargs)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "# merge datasets\n",
        "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
        "val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "hidden_states = [256]\n",
        "\n",
        "for hidden_size in hidden_states:\n",
        "    for rnn_type in rnn_types:\n",
        "        print(rnn_type, hidden_size)\n",
        "        model = CustomRNN(input_size, hidden_size, output_size, rnn_type).to('cuda')\n",
        "        train_model(model, train_dataloader, val_dataloader, num_epochs=200, learning_rate=0.001, verbose=True)\n",
        "        plt.savefig(f'figs/{rnn_type}_{hidden_size}_mixed_0.15.png')\n",
        "        print('-' * 80)"
      ],
      "metadata": {
        "id": "86zOOsp_AryR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = []\n",
        "val_datasets = []\n",
        "\n",
        "batch_size = 4096\n",
        "\n",
        "for name, (dataset_class, kwargs) in dataloaders.items():\n",
        "    train_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0, pad_to=6, add_noise=True, **kwargs)\n",
        "    train_datasets.append(train_dataset)\n",
        "\n",
        "    val_dataset = dataset_class(dataset_size=batch_size, category_size=4, identity_size=2, std=0.35, pad_to=6, add_noise=True, **kwargs)\n",
        "    val_datasets.append(val_dataset)\n",
        "\n",
        "# merge datasets\n",
        "train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
        "val_dataset = torch.utils.data.ConcatDataset(val_datasets)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "hidden_states = [256]\n",
        "\n",
        "for hidden_size in hidden_states:\n",
        "    for rnn_type in rnn_types:\n",
        "        print(rnn_type, hidden_size)\n",
        "        model = CustomRNN(input_size, hidden_size, output_size, rnn_type).to('cuda')\n",
        "        train_model(model, train_dataloader, val_dataloader, num_epochs=150, learning_rate=0.001, verbose=True)\n",
        "        plt.savefig(f'figs/{rnn_type}_{hidden_size}_mixed_0.35.png')\n",
        "        print('-' * 80)"
      ],
      "metadata": {
        "id": "4qjTkBo8A_DW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}